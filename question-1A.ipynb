{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046ee135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Today', 'is', 'our', 'dragon', 'boat', 'festival', 'in', 'our', 'Chinese', 'culture', 'to', 'celebrate', 'it', 'with', 'all', 'safe', 'and', 'great', 'in', 'our', 'lives']\n",
      "['Anyway', 'I', 'believe', 'the', 'team', 'although', 'bit', 'delay', 'and', 'less', 'communication', 'at', 'recent', 'days', 'they', 'really', 'tried', 'best', 'for', 'paper', 'and', 'cooperation']\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "import pandas as pd\n",
    "\n",
    "# Initializing the Stanza pipeline\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,lemma,depparse')\n",
    "\n",
    "\n",
    "# Original sentences - Highly likely to be Chinese to English direct translations\n",
    "sentence1 = \"Today is our dragon boat festival, in our Chinese culture, to celebrate it with all safe and great in our lives.\"\n",
    "sentence2 = \"Anyway, I believe the team, although bit delay and less communication at recent days, they really tried best for paper and cooperation.\"\n",
    "\n",
    "\n",
    "# To help us with the grammar substitutions, POS tagging from stanza was used to identify the parts of speech of each word in the sentences\n",
    "def stanza_pos_parser(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    tokens = [word.text for sent in doc.sentences for word in sent.words]\n",
    "    pos_tags = [(word.text,word.upos) for sent in doc.sentences for word in sent.words] \n",
    "\n",
    "    print(\"Stanza pipeline tokenization And Part Of Speech Tagging\")\n",
    "    return tokens, pos_tags\n",
    "\n",
    "# Parsing the sentences to get tokens and POS tags\n",
    "tokens1, pos_tags1 = stanza_pos_parser(sentence1) \n",
    "tokens2, pos_tags2 = stanza_pos_parser(sentence2) \n",
    "\n",
    "# Using pandas to create a DataFrame for better visualization of the POS tags\n",
    "def print_pos_tags(pos_tags):\n",
    "    stanza_dataframe = pd.DataFrame({\n",
    "        'Token': [token for token, pos in pos_tags],\n",
    "        'POS': [pos for token, pos in pos_tags]\n",
    "    })\n",
    "    \n",
    "    print(stanza_dataframe) \n",
    "\n",
    "# print_pos_tags(pos_tags1)\n",
    "# print_pos_tags(pos_tags2)\n",
    "\n",
    "\n",
    "# Mostly phrase substitutions in Chinese to English direct translation, otherwise known as \"Chinglish\"\n",
    "phrase_substitutions = {\n",
    "    (\"although\", \"bit\", \"delay\"): [\"regardless\", \"of\", \"the\", \"delays\"],\n",
    "    (\"tried\", \"best\"): [\"tried\", \"their\", \"best\"],\n",
    "    (\"at\", \"recent\", \"days\"): [\"recently\"],\n",
    "    (\"for\", \"paper\", \"and\", \"cooperation\"): [\"on\", \"the\", \"paper\", \"and\", \"contributed\", \"to\", \"our\", \"collaboration\"],\n",
    "    (\"our\", \"dragon\", \"boat\", \"festival\") : [\"the\", \"Dragon\", \"Boat\", \"Festival\"],\n",
    "    (\"with\", \"all\", \"safe\", \"and\", \"great\", \"in\", \"our\", \"lives\"): [\"to\", \"wish\", \"for\", \"safety\", \"and\", \"prosperity\"],\n",
    "}\n",
    "\n",
    "# Grammar-based custom substitution / modification rules \n",
    "grammar_substitutions = [\n",
    "    {\n",
    "        \"word\": \"to\",               # Target word to replace (hence, we have a replacement)\n",
    "        \"lemma\": \"to\",              # Base form of the word\n",
    "        \"upos\": \"PART\",             # POS tag: particle \n",
    "        \"deprel\": \"mark\",           # Dependency relation: marker (e.g., \"to celebrate\")\n",
    "        \"head_lemma\": \"celebrate\",  # Ensure \"to\" is governed by the verb \"celebrate\"\n",
    "        \"replacement\": [\"we\"]\n",
    "    },\n",
    "    {\n",
    "        \"word\": \"our\",              # Target word to remove (hence, replacement is empty)\n",
    "        \"lemma\": \"our\",            \n",
    "        \"upos\": \"DET\",              # POS tag: determiner \n",
    "        \"deprel\": \"det\",            # Dependency relation: determiner (e.g., \"our culture\")\n",
    "        \"head_lemma\": \"culture\",    # Ensure \"our\" is governed by the word \"culture\"\n",
    "        \"replacement\": []         \n",
    "    },\n",
    "    {\n",
    "        \"word\": \"less\",\n",
    "        \"lemma\": \"less\",\n",
    "        \"upos\": \"ADJ\",                  # POS tag: adjective\n",
    "        \"deprel\": \"amod\",               # Dependency relation: adjectival modifier\n",
    "        \"head_lemma\": \"communication\",  # Ensure \"less\" modifies \"communication\"\n",
    "        \"replacement\": [\"limited\"]      # Replace \"less\" with \"limited\"\n",
    "    },\n",
    "    {\n",
    "        \"word\": \"they\",             # Target word to remove (hence, replacement is empty)\n",
    "        \"lemma\": \"they\",           \n",
    "        \"upos\": \"PRON\",             # POS tag: pronoun\n",
    "        \"deprel\": \"nsubj\",          # Dependency relation: nominal subject (e.g., \"they tried\")\n",
    "        \"head_lemma\": \"try\",        # Ensure \"they\" is governed by the verb \"try\"\n",
    "        \"replacement\": []\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "# Function for applying phrase substitutions\n",
    "def apply_phrase_substitutions(tokens):\n",
    "    output = []\n",
    "    i = 0\n",
    "    \n",
    "    # Sorting phrase substitutions by length of phrase in descending order, in order to match longer phrases first and avoid partial matches.\n",
    "    # This is important since we want to prevent cases where a shorter phrase is matched before a longer one that may contain it.\n",
    "    sorted_substitutions = sorted(phrase_substitutions.items(), key=lambda x: -len(x[0]))\n",
    "    \n",
    "    # We iterate through the tokens of the sentence\n",
    "    while i < len(tokens):\n",
    "        \n",
    "        # Boolean variable to check if a match was found\n",
    "        matched = False\n",
    "        \n",
    "        # For each phrase and its replacement, we check if the current sentence token matches it\n",
    "        for phrase, replacement in sorted_substitutions:\n",
    "            \n",
    "            # If we have a match, we extend the output with the replacement \n",
    "            # and increment the index by the length of the phrase and break the loop since we took care of this phrase\n",
    "            if tuple(tokens[i:i + len(phrase)]) == phrase:\n",
    "                output.extend(replacement)\n",
    "                i += len(phrase)\n",
    "                matched = True\n",
    "                break\n",
    "             \n",
    "        # If we had no match, we append the current unchanged token to the output and increment the index by 1\n",
    "        if not matched:\n",
    "            output.append(tokens[i])\n",
    "            i += 1\n",
    "            \n",
    "    # We return the output with all the phrase substitutions applied             \n",
    "    return output\n",
    "    \n",
    "    \n",
    "# Function for applying grammar-based substitutions\n",
    "def apply_grammar_substitutions(sentence):\n",
    "    output = []\n",
    "    \n",
    "    # We iterate through each word in the sentence\n",
    "    for word in sentence.words:\n",
    "        \n",
    "        # Boolean variable to check if a match with a rule was found\n",
    "        match = False\n",
    "        \n",
    "        # We check each for each substitution rule in the grammar substitutions \n",
    "        for substitution in grammar_substitutions:\n",
    "            \n",
    "            # We check if the word text matches the target word in the substitution rule\n",
    "            # For each check, if there is no match, we skip to the next substitution rule\n",
    "            if word.text.lower() != substitution.get(\"word\", word.text.lower()):\n",
    "                continue\n",
    "            \n",
    "            # We check the lemma of the word against the substitution rule\n",
    "            if \"lemma\" in substitution and word.lemma != substitution[\"lemma\"]:\n",
    "                continue\n",
    "            \n",
    "            # We check the universal part-of-speech tag\n",
    "            if \"upos\" in substitution and word.upos != substitution[\"upos\"]:\n",
    "                continue\n",
    "            \n",
    "            # We check the dependency relation\n",
    "            if \"deprel\" in substitution and word.deprel != substitution[\"deprel\"]:\n",
    "                continue\n",
    "            \n",
    "            # We check the head lemma to ensure the word we are modifying is the one we want\n",
    "            if \"head_lemma\" in substitution:\n",
    "                \n",
    "                # If the word has no head, we skip to the next substitution rule\n",
    "                # Essentially, this means we are looking for a specific context where the word is used in\n",
    "                head = sentence.words[word.head - 1] if word.head > 0 else None\n",
    "                if not head or head.lemma != substitution[\"head_lemma\"]:\n",
    "                    continue\n",
    "            \n",
    "            # We apply the changes by extending the output if all conditions met and break the loop\n",
    "            output.extend(substitution[\"replacement\"])\n",
    "            match = True\n",
    "            break\n",
    "        \n",
    "        # If there was no match, we append the original word to the output\n",
    "        if not match:\n",
    "            output.append(word.text)\n",
    "    \n",
    "    # We return the output with all the grammar substitutions applied\n",
    "    return output\n",
    "\n",
    "\n",
    "# Function for reconstructing the sentence with all the changes applied\n",
    "def reconstruct(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    final_tokens = []\n",
    "    \n",
    "    # We apply all the grammar and phrase substitutions made and reconstruct the sentence\n",
    "    for sentence in doc.sentences:\n",
    "        fixed_grammar = apply_grammar_substitutions(sentence)\n",
    "        fixed_phrases = apply_phrase_substitutions(fixed_grammar)\n",
    "        final_tokens.extend(fixed_phrases)\n",
    "        \n",
    "    # We join the final tokens into a single string and return it\n",
    "    return \" \".join(final_tokens)\n",
    "\n",
    "\n",
    "# Reconstruction of the sentences - Printing the both the original and the reconstructed sentences\n",
    "print(\"Original Sentence 1\")\n",
    "print(sentence1)\n",
    "print(\"Reconstructed Sentence 1\")\n",
    "print(reconstruct(sentence1))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Original Sentence 2\")\n",
    "print(sentence2)\n",
    "print(\"Reconstructed Sentence 2\")\n",
    "print(reconstruct(sentence2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97780e90",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stanza'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstanza\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Download once if needed\u001b[39;00m\n\u001b[32m      4\u001b[39m stanza.download(\u001b[33m'\u001b[39m\u001b[33men\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'stanza'"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "# Download once if needed\n",
    "stanza.download('en')\n",
    "\n",
    "# Initialize pipeline\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,lemma,depparse')\n",
    "\n",
    "def reconstruct(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    output_tokens = []\n",
    "\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            # Replace awkward words/phrases based on lemma and dependency\n",
    "            if word.text.lower() == \"bit\" and word.head and word.head.lemma == \"delay\":\n",
    "                output_tokens.extend([\"a\", \"bit\", \"of\", \"a\", \"delay\"])\n",
    "                continue\n",
    "\n",
    "            if word.text.lower() == \"less\" and word.head and word.head.lemma == \"communication\":\n",
    "                output_tokens.append(\"reduced\")\n",
    "                continue\n",
    "\n",
    "            if word.text.lower() == \"best\" and word.head and word.head.lemma == \"try\":\n",
    "                output_tokens.extend([\"their\", \"best\"])\n",
    "                continue\n",
    "\n",
    "            if word.text.lower() == \"it\" and word.deprel == \"obj\":\n",
    "                output_tokens.append(\"the festival\")\n",
    "                continue\n",
    "\n",
    "            output_tokens.append(word.text)\n",
    "\n",
    "    # Capitalize first word\n",
    "    if output_tokens:\n",
    "        output_tokens[0] = output_tokens[0].capitalize()\n",
    "\n",
    "    return \" \".join(output_tokens)\n",
    "\n",
    "\n",
    "# Original sentences\n",
    "sentence1 = \"Today is our dragon boat festival, in our Chinese culture, to celebrate it with all safe and great in our lives.\"\n",
    "sentence2 = \"Anyway, I believe the team, although bit delay and less communication at recent days, they really tried best for paper and cooperation.\"\n",
    "\n",
    "# Run reconstructions\n",
    "reconstructed1 = reconstruct(sentence1)\n",
    "reconstructed2 = reconstruct(sentence2)\n",
    "\n",
    "print(\"Reconstructed 1:\")\n",
    "print(reconstructed1)\n",
    "print(\"\\nReconstructed 2:\")\n",
    "print(reconstructed2)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
